{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYOF6Re5SwQA"
   },
   "source": [
    "## Text Generation using LSTM\n",
    "\n",
    "Text generation is the task of generating text with the goal of appearing indistinguishable to human-written text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![topic_modeling](text_gen.png)](https://github.com/scionoftech/Text_Generation_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuIzpYc1SrxI"
   },
   "source": [
    "LSTM (Long Short Term Memory) are very good for analysing sequences of values and predicting the next values from them. For example, LSTM could be a very good choice if you want to predict the very next point of a given time serie (assuming a correlation exist in the sequence).\n",
    "\n",
    "Talking about sentences and texts ; phrases (sentences) are basically sequences of words. So, it is natural to assume LSTM could be usefull to generate the next word of a given sentence.\n",
    "\n",
    "In summary, the objective of a LSTM neural network in this situation is to guess the next word of a given sentence.\n",
    "\n",
    "For example: What is the next word of this following sentence : \"he is walking down the\"\n",
    "\n",
    "Our neural net will take the sequence of words as input : \"he\", \"is\", \"walking\", ... Its ouput will be a matrix providing the probability for each word from the dictionnary to be the next one of the given sentence.\n",
    "\n",
    "Then, how will we build the complete text ? Simply iterating the process, by switching the setence by one word, including the new guessed word at its end. Then, we guess a new word for this new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pi0Q_HhRR9KT"
   },
   "source": [
    "Process\n",
    "In order to do that, first, we build a dictionary containing all words from the novels we want to use.\n",
    "\n",
    "* read the data (the novels we want to use),\n",
    "* create the dictionnary of words,\n",
    "* create the list of sentences,\n",
    "* create the neural network,\n",
    "* train the neural network,\n",
    "* generate new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "mIggi2a6YvkX",
    "outputId": "fa2d8138-49f2-4337-97a8-1f3b17624a92"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn import model_selection, preprocessing\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fDRRDUQPY4Lq",
    "outputId": "ddd67057-e16f-4d72-c000-13afb4bd1836"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujoWbiNwY5bQ"
   },
   "outputs": [],
   "source": [
    "project_path = \"/content/drive/My Drive/DLCP/openwork/text_generation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKh_dA-tf-VC"
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6oMb12pYvke"
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = project_path+\"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0NjrAIXHpBn"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # remove next lines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # filter to allow only alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    # remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "\n",
    "    text = ' '.join([w for w in text.split() if w not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0')])\n",
    "\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    text = text.replace('\"','')\n",
    "    text = text.replace('  ',' ')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDlo4qV-J9K7"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "corpus = clean_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0JmC5nCf4Af"
   },
   "source": [
    "### Create dictionnary\n",
    "The first step is to create the dictionnary, it means, the list of all words contained in texts. For each word, we will assign an index to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0LNrzJwEEsP"
   },
   "outputs": [],
   "source": [
    "wordlist = corpus.split()\n",
    "\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g2sofKzHCCg4",
    "outputId": "24bd03c1-c072-48e9-c1b4-039233396281"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  3056\n"
     ]
    }
   ],
   "source": [
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMHeUH4SgGg2"
   },
   "source": [
    "### create sequences\n",
    "\n",
    "Now, we have to create the input data for our LSTM. We create two lists:\n",
    "\n",
    "* **sequences**: this list will contain the sequences of words used to train the model,\n",
    "* **next_words**: this list will contain the next words for each sequences of the **sequences** list.\n",
    "In this exercice, we assume we will train the network with sequences of 30 words (seq_length = 30).\n",
    "\n",
    "So, to create the first sequence of words, we take the 30th first words in the **wordlist** list. The word 31 is the next word of this first sequence, and is added to the **next_words** list.\n",
    "\n",
    "Then we jump by a step of 1 (sequences_step = 1 in our example) in the list of words, to create the second sequence of words and retrieve the second \"next word\".\n",
    "\n",
    "We iterate this task until the end of the list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aIbXBJUQPcaP",
    "outputId": "80e914b9-7f36-4dcf-8db7-540c1cfa9510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 29729\n"
     ]
    }
   ],
   "source": [
    "seq_length = 30 # sequence length\n",
    "sequences_step = 1 #step to create sequences\n",
    "\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZHmokUGN2ou"
   },
   "source": [
    "When we iterate over the whole list of words, we create 30844 sequences of words, and retrieve, for each of them, the next word to be predicted.\n",
    "\n",
    "However, these lists cannot be used \"as is\". We have to transform them in order to ingest them in the LSTM. Text will not be understood by neural net, we have to use digits. However, we cannot only map a words to its index in the vocabulary, as it does not represent intrasinqly the word. It is better to reorganize a sequence of words as a matrix of booleans.\n",
    "\n",
    "So, we create the matrix X and y :\n",
    "\n",
    "X : the matrix of the following dimensions:\n",
    "number of sequences,\n",
    "number of words in sequences,\n",
    "number of words in the vocabulary.\n",
    "y : the matrix of the following dimensions:\n",
    "number of sequences,\n",
    "number of words in the vocabulary.\n",
    "For each word, we retrieve its index in the vocabulary, and we set to 1 its position in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f4EjM3QuOBTv"
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEigEi9Mgm3z"
   },
   "source": [
    "Now, here come the fun part. The creation of the neural network. As you will see, I am using Keras which provide very good abstraction to design an architecture.\n",
    "\n",
    "In this example, I create the following neural network:\n",
    "\n",
    "* bidirectional LSTM,\n",
    "* with size of 256 and using RELU as activation,\n",
    "* then a dropout layer of 0,6 (it's pretty high, but necesseray to avoid quick divergence)\n",
    "\n",
    "The net should provide me a probability for each word of the vocabulary to be the next one after a given sentence. So I end it with:\n",
    "\n",
    "* a simple dense layer of the size of the vocabulary,\n",
    "* a softmax activation.\n",
    "\n",
    "I use ADAM as otpimizer and the loss calculation is done on the categorical crossentropy.\n",
    "\n",
    "Here is the function to build the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvgYAF-iOyxu"
   },
   "outputs": [],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "batch_size = 128 # minibatch size\n",
    "seq_length = 30 # sequence length\n",
    "num_epochs = 50 # number of epochs\n",
    "learning_rate = 0.01 #learning rate\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "QCpY6e19LIXe",
    "outputId": "661f3cdb-2634-4f1b-b3c1-81241ca04004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 512)               6785024   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3056)              1567728   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 3056)              0         \n",
      "=================================================================\n",
      "Total params: 8,352,752\n",
      "Trainable params: 8,352,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build LSTM model.')\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "model.add(tf.keras.layers.Dropout(0.6))\n",
    "model.add(tf.keras.layers.Dense(vocab_size))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTuwHPkpg-IA"
   },
   "source": [
    "### train data\n",
    "Enough speech, we train the model now. We shuffle the training set and extract 10% of it as validation sample. We simply run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "s2u2R_v2PRWU",
    "outputId": "907b8694-9d05-4f73-9a1f-a03d8f2c08ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 29729 samples\n",
      "Epoch 1/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 28070.6728\n",
      "Epoch 00001: loss improved from inf to 28039.52111, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 28s 957us/sample - loss: 28039.5211\n",
      "Epoch 2/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 6.8254\n",
      "Epoch 00002: loss improved from 28039.52111 to 6.82531, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 885us/sample - loss: 6.8253\n",
      "Epoch 3/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.9962\n",
      "Epoch 00003: loss improved from 6.82531 to 5.99549, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 5.9955\n",
      "Epoch 4/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 12.8605\n",
      "Epoch 00004: loss did not improve from 5.99549\n",
      "29729/29729 [==============================] - 25s 857us/sample - loss: 12.8524\n",
      "Epoch 5/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.4086\n",
      "Epoch 00005: loss improved from 5.99549 to 5.40828, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 885us/sample - loss: 5.4083\n",
      "Epoch 6/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.3200\n",
      "Epoch 00006: loss improved from 5.40828 to 5.32013, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 5.3201\n",
      "Epoch 7/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.2654\n",
      "Epoch 00007: loss improved from 5.32013 to 5.26553, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 880us/sample - loss: 5.2655\n",
      "Epoch 8/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.2026\n",
      "Epoch 00008: loss improved from 5.26553 to 5.20265, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 5.2027\n",
      "Epoch 9/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.1530\n",
      "Epoch 00009: loss improved from 5.20265 to 5.15297, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 887us/sample - loss: 5.1530\n",
      "Epoch 10/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.0773\n",
      "Epoch 00010: loss improved from 5.15297 to 5.07749, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 885us/sample - loss: 5.0775\n",
      "Epoch 11/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 5.0154\n",
      "Epoch 00011: loss improved from 5.07749 to 5.01516, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 5.0152\n",
      "Epoch 12/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.9622\n",
      "Epoch 00012: loss improved from 5.01516 to 4.96219, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 885us/sample - loss: 4.9622\n",
      "Epoch 13/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.9055\n",
      "Epoch 00013: loss improved from 4.96219 to 4.90585, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 4.9059\n",
      "Epoch 14/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.8406\n",
      "Epoch 00014: loss improved from 4.90585 to 4.84047, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 4.8405\n",
      "Epoch 15/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.7851\n",
      "Epoch 00015: loss improved from 4.84047 to 4.78557, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 886us/sample - loss: 4.7856\n",
      "Epoch 16/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.7329\n",
      "Epoch 00016: loss improved from 4.78557 to 4.73310, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 4.7331\n",
      "Epoch 17/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.6552\n",
      "Epoch 00017: loss improved from 4.73310 to 4.65588, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 4.6559\n",
      "Epoch 18/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.6070\n",
      "Epoch 00018: loss improved from 4.65588 to 4.60651, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 4.6065\n",
      "Epoch 19/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.5431\n",
      "Epoch 00019: loss improved from 4.60651 to 4.54223, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 4.5422\n",
      "Epoch 20/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.5053\n",
      "Epoch 00020: loss improved from 4.54223 to 4.50544, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 27s 893us/sample - loss: 4.5054\n",
      "Epoch 21/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.4542\n",
      "Epoch 00021: loss improved from 4.50544 to 4.45436, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 4.4544\n",
      "Epoch 22/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.3912\n",
      "Epoch 00022: loss improved from 4.45436 to 4.39172, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 877us/sample - loss: 4.3917\n",
      "Epoch 23/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.3330\n",
      "Epoch 00023: loss improved from 4.39172 to 4.33258, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 879us/sample - loss: 4.3326\n",
      "Epoch 24/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.2853\n",
      "Epoch 00024: loss improved from 4.33258 to 4.28541, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 4.2854\n",
      "Epoch 25/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.2249\n",
      "Epoch 00025: loss improved from 4.28541 to 4.22514, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 4.2251\n",
      "Epoch 26/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.1735\n",
      "Epoch 00026: loss improved from 4.22514 to 4.17340, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 885us/sample - loss: 4.1734\n",
      "Epoch 27/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.1231\n",
      "Epoch 00027: loss improved from 4.17340 to 4.12280, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 4.1228\n",
      "Epoch 28/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.0671\n",
      "Epoch 00028: loss improved from 4.12280 to 4.06653, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 880us/sample - loss: 4.0665\n",
      "Epoch 29/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 4.0342\n",
      "Epoch 00029: loss improved from 4.06653 to 4.03458, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 27s 920us/sample - loss: 4.0346\n",
      "Epoch 30/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.9844\n",
      "Epoch 00030: loss improved from 4.03458 to 3.98437, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 887us/sample - loss: 3.9844\n",
      "Epoch 31/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.9251\n",
      "Epoch 00031: loss improved from 3.98437 to 3.92567, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 3.9257\n",
      "Epoch 32/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.8868\n",
      "Epoch 00032: loss improved from 3.92567 to 3.88668, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 886us/sample - loss: 3.8867\n",
      "Epoch 33/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.8424\n",
      "Epoch 00033: loss improved from 3.88668 to 3.84310, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 891us/sample - loss: 3.8431\n",
      "Epoch 34/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.7874\n",
      "Epoch 00034: loss improved from 3.84310 to 3.78770, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 3.7877\n",
      "Epoch 35/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.7474\n",
      "Epoch 00035: loss improved from 3.78770 to 3.74743, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 3.7474\n",
      "Epoch 36/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.7147\n",
      "Epoch 00036: loss improved from 3.74743 to 3.71474, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 888us/sample - loss: 3.7147\n",
      "Epoch 37/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.6605\n",
      "Epoch 00037: loss improved from 3.71474 to 3.66061, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 3.6606\n",
      "Epoch 38/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.6216\n",
      "Epoch 00038: loss improved from 3.66061 to 3.62201, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 877us/sample - loss: 3.6220\n",
      "Epoch 39/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.5780\n",
      "Epoch 00039: loss improved from 3.62201 to 3.57858, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 3.5786\n",
      "Epoch 40/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.5420\n",
      "Epoch 00040: loss improved from 3.57858 to 3.54205, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 879us/sample - loss: 3.5420\n",
      "Epoch 41/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.5048\n",
      "Epoch 00041: loss improved from 3.54205 to 3.50436, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 3.5044\n",
      "Epoch 42/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.4537\n",
      "Epoch 00042: loss improved from 3.50436 to 3.45354, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 3.4535\n",
      "Epoch 43/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.4263\n",
      "Epoch 00043: loss improved from 3.45354 to 3.42639, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 3.4264\n",
      "Epoch 44/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.3913\n",
      "Epoch 00044: loss improved from 3.42639 to 3.39151, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 888us/sample - loss: 3.3915\n",
      "Epoch 45/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.3449\n",
      "Epoch 00045: loss improved from 3.39151 to 3.34512, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 884us/sample - loss: 3.3451\n",
      "Epoch 46/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.2933\n",
      "Epoch 00046: loss improved from 3.34512 to 3.29421, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 881us/sample - loss: 3.2942\n",
      "Epoch 47/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.2628\n",
      "Epoch 00047: loss improved from 3.29421 to 3.26359, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 880us/sample - loss: 3.2636\n",
      "Epoch 48/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.2343\n",
      "Epoch 00048: loss improved from 3.26359 to 3.23399, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 27s 896us/sample - loss: 3.2340\n",
      "Epoch 49/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.1865\n",
      "Epoch 00049: loss improved from 3.23399 to 3.18611, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 882us/sample - loss: 3.1861\n",
      "Epoch 50/50\n",
      "29696/29729 [============================>.] - ETA: 0s - loss: 3.1622\n",
      "Epoch 00050: loss improved from 3.18611 to 3.16180, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_vec_best.hdf5\n",
      "29729/29729 [==============================] - 26s 883us/sample - loss: 3.1618\n"
     ]
    }
   ],
   "source": [
    "# Defining a helper function to save the model after each epoch \n",
    "# in which the loss decreases \n",
    "filepath = project_path+\"text_generation_word_vec_best.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor ='loss', \n",
    "\t\t\t\t\t\t\tverbose = 1, save_best_only = True, \n",
    "\t\t\t\t\t\t\tmode ='min') \n",
    "earlystop = tf.keras.callbacks.EarlyStopping(patience=4, monitor='loss')\n",
    "# Defining a helper function to reduce the learning rate each time \n",
    "# the learning plateaus \n",
    "reduce_alpha = tf.keras.callbacks.ReduceLROnPlateau(monitor ='loss', factor = 0.2, \n",
    "\t\t\t\t\t\t\tpatience = 1, min_lr = 0.001) \n",
    "# callbacks = [print_callback, checkpoint, reduce_alpha] \n",
    "callbacks = [checkpoint,earlystop, reduce_alpha] \n",
    "\n",
    "history = model.fit(X, y,batch_size=batch_size,epochs=num_epochs,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpzYgMtDPPTj"
   },
   "outputs": [],
   "source": [
    "filepath=project_path+\"text_generation_word_vec.hdf5\"\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUxTdxXpPRxj"
   },
   "outputs": [],
   "source": [
    "# if os.path.isfile(filepath):\n",
    "#      model = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ih6wVsuwhI7G"
   },
   "source": [
    "# Generate phrase\n",
    "\n",
    "Great ! We have now trained a model to predict the next word of a given sequence of words. In order to generate text, the task is pretty simple:\n",
    "\n",
    "we define a \"seed\" sequence of 30 words (30 is the number of words required by the neural net for the sequences), we ask the neural net to predict word number 31,\n",
    "then we update the sequence by moving words by a step of 1, adding words number 31 at its end, we ask the neural net to predict word number 32, etc. For as long as we want.\n",
    "\n",
    "Doing this, we generate phrases, word by word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bWw1m_6hiUJ"
   },
   "source": [
    "To improve the word generation, and tune a bit the prediction, we introduce a specific function to pick-up words.\n",
    "\n",
    "We will not take the words with the highest prediction (or the generation of text will be boring), but would like to insert some uncertainties, and let the solution sometime pick-up words with less good prediction.\n",
    "\n",
    "That is the purpose of the function sample, that will draw radomly a word from the vocabulary.\n",
    "\n",
    "The probabilty for a word to be drawn will depends directly on its probability to be the next word. In order to tune this probability, we introduce a \"temperature\" to smooth or sharpen its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LF39lG93eUQl"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "4qVPvnA6eX2I",
    "outputId": "9ebed0d4-e8ba-42eb-d9ba-a8140900bea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \"a a a a a a a It was all very well to say 'Drink me,' but the wise little Alice was not going to do THAT in a hurry.\"\n"
     ]
    }
   ],
   "source": [
    "#initiate sentences\n",
    "seed_sentences = \"It was all very well to say 'Drink me,' but the wise little Alice was not going to do THAT in a hurry.\" \n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "sequence = clean_text(generated)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gpZS12NJexjj",
    "outputId": "a48c9d7a-488c-461e-b08d-c05e617088fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a It was all very well to say 'Drink me,' but the wise little Alice was not going to do THAT in a hurry. the its its the a a a the a the it its the she the the the the the a its the a the the a the a the the the the its its the it it it the the a its the its the the its she it and the the she a a the the the its the a the the a the a the a the the the the she the its the a the the the the the the a its its its the the the the the the she its it the the the it\n"
     ]
    }
   ],
   "source": [
    "words_number = 100\n",
    "#generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sequence.split()):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.34)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSKhBgKY5PKW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Text_generation_word_vec_BEST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
