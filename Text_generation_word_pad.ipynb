{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJO8FuFBI_68"
   },
   "source": [
    "# Text Generation\n",
    "\n",
    "Text generation is the task of generating text with the goal of appearing indistinguishable to human-written text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![topic_modeling](text_gen.png)](https://github.com/scionoftech/Text_Generation_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyLZofMvH4Mb"
   },
   "source": [
    "LSTM (Long Short Term Memory) are very good for analysing sequences of values and predicting the next values from them. For example, LSTM could be a very good choice if you want to predict the very next point of a given time serie (assuming a correlation exist in the sequence).\n",
    "\n",
    "Talking about sentences and texts ; phrases (sentences) are basically sequences of words. So, it is natural to assume LSTM could be usefull to generate the next word of a given sentence.\n",
    "\n",
    "In summary, the objective of a LSTM neural network in this situation is to guess the next word of a given sentence.\n",
    "\n",
    "For example: What is the next word of this following sentence : \"he is walking down the\"\n",
    "\n",
    "Our neural net will take the sequence of words as input : \"he\", \"is\", \"walking\", ... Its ouput will be a matrix providing the probability for each word from the dictionnary to be the next one of the given sentence.\n",
    "\n",
    "Then, how will we build the complete text ? Simply iterating the process, by switching the setence by one word, including the new guessed word at its end. Then, we guess a new word for this new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kjUzwSQH7Ir"
   },
   "source": [
    "Process\n",
    "In order to do that, first, we build a dictionary containing all words from the novels we want to use.\n",
    "\n",
    "* read the data (the novels we want to use),\n",
    "* create the list of sentences,\n",
    "* sequence padding,\n",
    "* create the neural network,\n",
    "* train the neural network,\n",
    "* generate new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIggi2a6YvkX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn import model_selection, preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDRRDUQPY4Lq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujoWbiNwY5bQ"
   },
   "outputs": [],
   "source": [
    "project_path = \"/content/drive/My Drive/text_generation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6oMb12pYvke"
   },
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = project_path+\"siddartha_by_hermann_hesse.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0NjrAIXHpBn"
   },
   "outputs": [],
   "source": [
    "# text preprocessing\n",
    "def clean_text(text):\n",
    "\n",
    "    # remove next lines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # filter to allow only alphabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    \n",
    "    # remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # convert to lowercase to maintain consistency\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "corpus = clean_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qo2Gu0lwJCru"
   },
   "outputs": [],
   "source": [
    "# Generate input Sequence and out Sequence\n",
    "def generate_sequences(text,seq_length):\n",
    "    tokens= text.split()\n",
    "    seq_in = []\n",
    "    seq_out = []\n",
    "    for i in range(0, len(tokens) - seq_length, 1):\n",
    "        seq_in.append(' '.join(tokens[i:i+seq_length]))\n",
    "        seq_out.append(tokens[i+seq_length])\n",
    "        \n",
    "    return seq_in,seq_out\n",
    "    \n",
    "seq_in,seq_out = generate_sequences(corpus,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWwj2_KqVyN0"
   },
   "outputs": [],
   "source": [
    "# num_max = 1000\n",
    "max_len = 100\n",
    "\n",
    "# Tokenize\n",
    "tok_seq_in = tf.keras.preprocessing.text.Tokenizer()\n",
    "tok_seq_in.fit_on_texts(seq_in)\n",
    "vocab_size_seq_in = len(tok_seq_in.word_index) + 1\n",
    "\n",
    "# sequence padding\n",
    "def get_seq_in_pad_sequences(seq_in):\n",
    "\n",
    "    # for cnn preproces\n",
    "    texts_seq = tok_seq_in.texts_to_sequences(seq_in)\n",
    "\n",
    "    texts_mat = tf.keras.preprocessing.sequence.pad_sequences(texts_seq,maxlen=max_len,padding='post')\n",
    "\n",
    "    return np.asarray(texts_mat)\n",
    "\n",
    "X_in = get_seq_in_pad_sequences(seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eqmpCZAQzij"
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of output sequence\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(seq_out)\n",
    "\n",
    "def encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return tf.keras.utils.to_categorical(enc)\n",
    "\n",
    "def decode(le, one_hot):\n",
    "    # dec = np.argmax(one_hot, axis=1)\n",
    "    dec = np.argmax(one_hot[0])\n",
    "    return le.inverse_transform((dec,))[0]\n",
    "y_enc = encode(le, seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apKELhobKogt"
   },
   "source": [
    "### Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "Q4DsA_BbYvkw",
    "outputId": "621d64dc-e236-46b3-8699-9a243fc1ff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          402400    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4006)              516774    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4006)              0         \n",
      "=================================================================\n",
      "Total params: 1,036,422\n",
      "Trainable params: 1,036,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "# Building the LSTM network for the task \n",
    "model = tf.keras.models.Sequential() \n",
    "model.add(tf.keras.layers.Embedding(vocab_size_seq_in,100, input_length=max_len))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.LSTM(128)) \n",
    "model.add(tf.keras.layers.Dense(y_enc.shape[1])) \n",
    "model.add(tf.keras.layers.Activation('softmax')) \n",
    "optimizer = tf.keras.optimizers.RMSprop(lr = 0.01) \n",
    "model.compile(loss ='categorical_crossentropy', optimizer = optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6krRGApTYvkz",
    "outputId": "44c05be4-8050-4ff7-ced5-956fa8f7d265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42278 samples\n",
      "Epoch 1/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.7128\n",
      "Epoch 00001: loss improved from inf to 4.71318, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.7132\n",
      "Epoch 2/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.6250\n",
      "Epoch 00002: loss improved from 4.71318 to 4.62542, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.6254\n",
      "Epoch 3/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.5605\n",
      "Epoch 00003: loss improved from 4.62542 to 4.56104, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 60s 1ms/sample - loss: 4.5610\n",
      "Epoch 4/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.5074\n",
      "Epoch 00004: loss improved from 4.56104 to 4.50768, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.5077\n",
      "Epoch 5/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.4697\n",
      "Epoch 00005: loss improved from 4.50768 to 4.47125, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.4712\n",
      "Epoch 6/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.4399\n",
      "Epoch 00006: loss improved from 4.47125 to 4.43938, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.4394\n",
      "Epoch 7/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.4006\n",
      "Epoch 00007: loss improved from 4.43938 to 4.40032, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.4003\n",
      "Epoch 8/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.3641\n",
      "Epoch 00008: loss improved from 4.40032 to 4.36456, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.3646\n",
      "Epoch 9/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.3364\n",
      "Epoch 00009: loss improved from 4.36456 to 4.33603, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.3360\n",
      "Epoch 10/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.3173\n",
      "Epoch 00010: loss improved from 4.33603 to 4.31680, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.3168\n",
      "Epoch 11/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.2884\n",
      "Epoch 00011: loss improved from 4.31680 to 4.28803, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.2880\n",
      "Epoch 12/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.2645\n",
      "Epoch 00012: loss improved from 4.28803 to 4.26493, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.2649\n",
      "Epoch 13/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.2314\n",
      "Epoch 00013: loss improved from 4.26493 to 4.23356, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.2336\n",
      "Epoch 14/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.2087\n",
      "Epoch 00014: loss improved from 4.23356 to 4.20835, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.2084\n",
      "Epoch 15/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.1717\n",
      "Epoch 00015: loss improved from 4.20835 to 4.17204, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.1720\n",
      "Epoch 16/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.1496\n",
      "Epoch 00016: loss improved from 4.17204 to 4.14989, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.1499\n",
      "Epoch 17/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.1226\n",
      "Epoch 00017: loss improved from 4.14989 to 4.12333, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.1233\n",
      "Epoch 18/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.0882\n",
      "Epoch 00018: loss improved from 4.12333 to 4.08820, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 4.0882\n",
      "Epoch 19/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.0547\n",
      "Epoch 00019: loss improved from 4.08820 to 4.05501, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.0550\n",
      "Epoch 20/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 4.0255\n",
      "Epoch 00020: loss improved from 4.05501 to 4.02561, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 4.0256\n",
      "Epoch 21/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.9805\n",
      "Epoch 00021: loss improved from 4.02561 to 3.98002, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.9800\n",
      "Epoch 22/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.9366\n",
      "Epoch 00022: loss improved from 3.98002 to 3.93594, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.9359\n",
      "Epoch 23/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.8955\n",
      "Epoch 00023: loss improved from 3.93594 to 3.89541, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.8954\n",
      "Epoch 24/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.8732\n",
      "Epoch 00024: loss improved from 3.89541 to 3.87310, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.8731\n",
      "Epoch 25/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.8398\n",
      "Epoch 00025: loss improved from 3.87310 to 3.83987, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.8399\n",
      "Epoch 26/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.8084\n",
      "Epoch 00026: loss improved from 3.83987 to 3.80869, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 3.8087\n",
      "Epoch 27/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.7878\n",
      "Epoch 00027: loss improved from 3.80869 to 3.78821, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.7882\n",
      "Epoch 28/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.7667\n",
      "Epoch 00028: loss improved from 3.78821 to 3.76736, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 62s 1ms/sample - loss: 3.7674\n",
      "Epoch 29/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.7467\n",
      "Epoch 00029: loss improved from 3.76736 to 3.74720, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.7472\n",
      "Epoch 30/30\n",
      "42240/42278 [============================>.] - ETA: 0s - loss: 3.7216\n",
      "Epoch 00030: loss improved from 3.74720 to 3.72183, saving model to /content/drive/My Drive/DLCP/openwork/text_generation/text_generation_word_pad.hdf5\n",
      "42278/42278 [==============================] - 61s 1ms/sample - loss: 3.7218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f76ee0bcbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a helper function to save the model after each epoch \n",
    "# in which the loss decreases \n",
    "filepath = project_path+\"text_generation_word_pad.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor ='loss', \n",
    "\t\t\t\t\t\t\tverbose = 1, save_best_only = True, \n",
    "\t\t\t\t\t\t\tmode ='min') \n",
    "\n",
    "# Defining a helper function to reduce the learning rate each time \n",
    "# the learning plateaus \n",
    "reduce_alpha = tf.keras.callbacks.ReduceLROnPlateau(monitor ='loss', factor = 0.2, \n",
    "\t\t\t\t\t\t\tpatience = 1, min_lr = 0.001) \n",
    "# callbacks = [print_callback, checkpoint, reduce_alpha] \n",
    "callbacks = [checkpoint, reduce_alpha] \n",
    "\n",
    "# fit the model\n",
    "model.fit(X_in, y_enc, epochs=30, batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6wz2yBoB3cm"
   },
   "outputs": [],
   "source": [
    "filepath=project_path+\"text_generation_word_pad.hdf5\"\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyr8Z90sCGAD"
   },
   "outputs": [],
   "source": [
    "# if os.path.isfile(filepath):\n",
    "#      model = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "RP3kHm8edJ0w",
    "outputId": "c67c14cf-4cda-453c-a306-2d0e434ee141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" respect from you you will learn it spoke vasudeva but not from me the river has taught me to listen \"\n",
      "to me to be a samana and i have been to be a single man i have been to be a single man i have learned to be a single one of the exalted one i have not a samana i have learned to be a single man i have been a samana i have learned to be a samana i have learned to be a samana i have learned to be able to do i do not know it is a samana i have learned to you and i have to find a rich man siddhartha is a samana i have learned to you you have learned to be a samana i have not learned and a stone i have learned to be a single man i have not be a samana i have not a single man i have been a samana i have learned to you and i have learned to be a single man i have learned to be able to do i have have learned to be a single man i have been to be a single man quoth siddhartha i have learned to you and i have learned to you travelling to me you are a samana and i have learned to be a samana i have learned to be able to do i have not a samana i have been to be a samana i have been to be long of a single man i have been to be a single man i have learned to be a samana i have not ascended a rich man i have not be a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a samana i have learned to you and a samana i have not a samana i have learned to be a samana i have been to be a single man i have not a samana i have been a samana i have learned to me and i have been to be a \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(seq_in)-1)\n",
    "pattern = seq_in[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", pattern, \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = get_seq_in_pad_sequences([pattern])\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tresult = decode(le,prediction)\n",
    "\tprint(result,end=' ')\n",
    "\tpts = pattern.split()\n",
    "\tpts.append(result)\n",
    "\tpattern = ' '.join(pts[1:len(pts)])\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SB4lz-pqk4Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_generation_word_pad_BEST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
